##### 正则化dropout

![image-20240303145007007](C:\Users\86186\AppData\Roaming\Typora\typora-user-images\image-20240303145007007.png)

每一个隐藏层随机存活，概率为p，

对于每个隐层的输出a，有

$p * a + (1 - p) * 0 =  pa$，激活值的期望为

$pa$，会让$w$满足激活值为pa时的权重。那么对于下一个隐层，

有$z = w_1 * p * a + ....$ 相较于以前 $z = w_1 * a$, $w_1会增大$，

$目前w_1是满足上一层激活值为pa情况下的参数$, 当测试时，不执行dropout，那么激活值为a，但是用到了pa时的参数，最后导致结果的期望大于训练时的期望。所以要保证期望不变，可以在训练时对激活值 / p，这样期望变为a，那么参数也就是对应于a的参数。或者在预测时，对应用了dropout的层使用乘p，那么输出为pa，训练的参数也恰好是为期望为pa准备的。

##### 梯度检验

$$
 lim_{\xi\to0} \frac{f(x + \xi) - f(x - \xi)}{ 2 * \xi} 
$$

利用双边进行估计导数，比单边误差值更小。

##### batch_size

一个batch的样本，求loss均值，然后根据均值去梯度下降更新权重。